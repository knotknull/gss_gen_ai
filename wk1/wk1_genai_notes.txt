#######################################################################
## Wk1: Gen AI Quickstart with Gemini & Vertex AI
#######################################################################

https://goo.gle/LuisLab


Created project under my account named 
 - ssgenai-gemini-vertex-ai 
        - enabled Vertex AI

## AI Studio 
aistudio.google.com

    - run a query with "Grounding with Google Search" checked performs a "RAG" using google search


## Vertex AI vs. AI Studio
 - AI Studio: research / test / experiment / etc.
 - Vertex AI: higher capacity, connectivity to other GCP Services  



# Vertex AI 
console > Vertex AI > Freeform




## Python GenAI SDK

https://googleapis.github.io/python-genai   # SDK Documentation
https://github.com/googleapis/python-genai  # Source Code




## Code to set client and call client to generate content

from google import genai
from google.genai import types
from IPython.display import Markdown

# Initialize the client by first determining whether or not to use Vertex AI
# by checking if "PROJECT_ID" is empty.
if PROJECT_ID == "":
  GOOGLE_API_KEY=userdata.get('AI_STUDIO_API_KEY')
  client = genai.Client(api_key=GOOGLE_API_KEY)
else:
  client = genai.Client(project=PROJECT_ID, location="us-central1", vertexai=True)


response = client.models.generate_content(
    model="gemini-2.0-flash-001",
    contents="What's the largest planet in our solar system? Tell me 3 facts about it."
)

Markdown(response.text)


## response object uses pydantic, 
## here is the response object
## 
print(response.model_dump_json(indent=2))

{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "video_metadata": null,
            "thought": null,
            "code_execution_result": null,
            "executable_code": null,
            "file_data": null,
            "function_call": null,
            "function_response": null,
            "inline_data": null,
            "text": "The largest planet in our solar system is **Jupiter**.\n\nHere are three facts about Jupiter:\n\n1.  **Great Red Spot:** Jupiter is famous for its Great Red Spot, a giant storm larger than Earth that has been raging for at least 350 years.\n2.  **Strong Magnetic Field:** Jupiter has the strongest planetary magnetic field in the Solar System. It's about 20,000 times stronger than Earth's.\n3.  **Many Moons:** Jupiter has a whopping 95 confirmed moons! The four largest (Io, Europa, Ganymede, and Callisto) are called the Galilean moons, discovered by Galileo Galilei in 1610.\n"
          }
        ],
        "role": "model"
      },
      "citation_metadata": null,
      "finish_message": null,
      "token_count": null,
      "avg_logprobs": -0.20402384764396103,
      "finish_reason": "STOP",
      "grounding_metadata": null,
      "index": null,
      "logprobs_result": null,
      "safety_ratings": null
    }
  ],
  "model_version": "gemini-2.0-flash-001",
  "prompt_feedback": null,
  "usage_metadata": {
    "cached_content_token_count": null,
    "candidates_token_count": 149,
    "prompt_token_count": 19,
    "total_token_count": 168
  },
  "automatic_function_calling_history": [],
  "parsed": null
}



## Streaming response from chat
## https://googleapis.github.io/python-genai/#chats
## NOTE: chat has memory 
##  context for Gemini Pro chat can be 1MM to 2MM tokens (depending on model)



chat = client.chats.create(model='gemini-2.0-flash-001')
for chunk in chat.send_message_stream("Tell me 3 facts about Jupiter and its moons"):
    print(chunk.text, end="")



Okay, here are 3 facts about Jupiter and its moons:

1.  **Jupiter has a moon with a subsurface ocean:** Europa, one of Jupiter's four largest moons (the Galilean moons), is believed to harbor a vast ocean of liquid water beneath its icy surface. This ocean is potentially habitable and makes Europa a prime target in the search for extraterrestrial life.

2.  **Io is the most volcanically active body in the Solar System:** Jupiter's moon Io is an incredibly active world, with hundreds of volcanoes constantly erupting. This extreme volcanism is due to tidal forces from Jupiter and the other Galilean moons, which flex and heat Io's interior.

3.  **Ganymede is the largest moon in the Solar System and has its own magnetic field:**  Ganymede, another Galilean moon, is larger than the planet Mercury.  Uniquely among moons, it also has its own magnetosphere, generated by a metallic core.




## Change the model (here it is using llama)
## NOTE: The model needs to be enabled first


response = client.models.generate_content(
    model="meta/llama-3.1-8b-instruct-maas",
    contents="What's the largest planet in our solar system? Tell me 3 facts about it."
)

Markdown(response.text)




## Can configure settings around prompt:
##  i.e. saftey, temperature, system instruction, etc.
## 

from IPython.display import Markdown

# The default safety settings should be sufficient for most scenarios.
# Optionally, developers can customize the safety settings.
# Learn more at:
# - https://googleapis.github.io/python-genai/index.html#safety-settings
# - https://cloud.google.com/vertex-ai/generative-ai/docs/learn/responsible-ai
CUSTOM_SAFETY_SETTINGS = [
    types.SafetySetting(
        category="HARM_CATEGORY_HATE_SPEECH",
        threshold="BLOCK_ONLY_HIGH",
    ),
    types.SafetySetting(
        category="HARM_CATEGORY_DANGEROUS_CONTENT",
        threshold="BLOCK_ONLY_HIGH",
    ),
    types.SafetySetting(
        category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
        threshold="BLOCK_ONLY_HIGH",
    ),
    types.SafetySetting(
        category="HARM_CATEGORY_HARASSMENT",
        threshold="BLOCK_ONLY_HIGH",
    ),
]

# Create a custom generation config to customize settings, such as temperature,
# response type, etc.
# - https://googleapis.github.io/python-genai/index.html#typed-config
MODEL_ID= 'gemini-2.0-flash-001' # @param ["gemini-2.0-flash-001", "gemini-2.0-flash-lite-preview-02-05", "gemini-2.0-pro-exp-02-05","gemini-2.0-flash-thinking-exp-01-21", "gemini-1.5-flash-002", "gemini-1.5-pro-002", "meta/llama-3.2-90b-vision-instruct-maas", "meta/llama-3.1-405b-instruct-maas","meta/llama-3.1-70b-instruct-maas","meta/llama-3.1-8b-instruct-maas"]
TEMPERATURE= 1 # @param {type:"slider", min:0, max:2, step:0.01}
TOP_P = 0.95 # @param {type:"slider", min:0, max:1, step:0.05}
TOP_K = 40 # @param {type:"slider", min:1, max:40, step:1}
MAX_OUTPUT_TOKENS = 8192 # @param {type:"slider", min:64, max:8192, step:64}
RESPONSE_TYPE = 'text/plain' # @param ["text/plain", "application/json'"]
SYSTEM_INSTRUCTION = "Speak like Shakespeare. Format your responses in markdown. Use lots of emoji." # @param {type:"string"}
PROMPT = "Tell me 3 facts about Jupiter." # @param {type:"string"}

CUSTOM_GENERATION_CONFIG = {
    "max_output_tokens": MAX_OUTPUT_TOKENS,
    "temperature": TEMPERATURE,
    "top_p": TOP_P,
    "top_k": TOP_K,
    "safety_settings": CUSTOM_SAFETY_SETTINGS,
    "responseMimeType": RESPONSE_TYPE,
    "system_instruction": SYSTEM_INSTRUCTION
}

response = client.models.generate_content(
    model=MODEL_ID,
    config=CUSTOM_GENERATION_CONFIG,
    contents=PROMPT
)

Markdown(response.text)


NOTES on TOP K and TOP P:

Top K:
Selects the "K" most probable words from the model's probability distribution to consider when choosing the next word. 
A lower "K" value leads to more predictable, conservative outputs by limiting the selection to highly probable words. 

Top P (also called Nucleus Sampling): 
Selects words until the cumulative probability of the chosen words reaches a specified "P" value. 
Allows for more flexibility in selecting words, potentially including less probable options while still maintaining a certain level of coherence. 




# Gemini is compatible with Open API SDK
# can use openai sdk to interact with Gemini
# NOTE: returned JSON structure is different 

from google.auth import default
import google.auth.transport.requests
import openai


# Programmatically get an access token
credentials, _ = default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
credentials.refresh(google.auth.transport.requests.Request())

# OpenAI Client
openai_client = openai.OpenAI(
    base_url=f"https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/openapi",
    api_key=credentials.token,
)

response = openai_client.chat.completions.create(
    model="google/gemini-2.0-flash-001",
    messages=[{"role": "user",
               "content": "Tell me 3 facts about Jupiter and its moons?"
               }],
)

print(response.to_json(indent=2))
print("---")
Markdown(response.choices[0].message.content)


{
  "id": "2025-02-27|07:12:51.520330-08|9.22.61.30|1849487733",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "logprobs": null,
      "message": {
        "content": "Okay, here are 3 facts about Jupiter and its moons:\n\n1.  **Jupiter has the largest ocean in the solar system, but it's not made of water.** Instead, it's believed to be a massive ocean of metallic hydrogen, formed under immense pressure from the planet's gravity. This ocean is thought to be a key factor in generating Jupiter's powerful magnetic field.\n\n2.  **Io, one of Jupiter's four largest moons (the Galilean moons), is the most volcanically active world in the Solar System.** It has hundreds of volcanoes, some erupting plumes of sulfur and sulfur dioxide hundreds of kilometers high. This extreme volcanism is due to tidal forces exerted by Jupiter and the other Galilean moons.\n\n3.  **Europa, another of Jupiter's Galilean moons, is believed to harbor a saltwater ocean beneath a thick icy crust.** Scientists believe this ocean could potentially contain twice as much water as all of Earth's oceans combined. Europa is a prime target in the search for extraterrestrial life because of this potential liquid water environment.\n",
        "role": "assistant"
      }
    }
  ],
  "created": 1740669171,
  "model": "google/gemini-2.0-flash-001",
  "object": "chat.completion",
  "system_fingerprint": "",
  "usage": {
    "completion_tokens": 223,
    "prompt_tokens": 11,
    "total_tokens": 234
  }
}

Okay, here are 3 facts about Jupiter and its moons:
    Jupiter has the largest ocean in the solar system, but it's not made of water. Instead, it's believed to be a massive ocean of metallic hydrogen, formed under immense pressure from the planet's gravity. This ocean is thought to be a key factor in generating Jupiter's powerful magnetic field.
    Io, one of Jupiter's four largest moons (the Galilean moons), is the most volcanically active world in the Solar System. It has hundreds of volcanoes, some erupting plumes of sulfur and sulfur dioxide hundreds of kilometers high. This extreme volcanism is due to tidal forces exerted by Jupiter and the other Galilean moons.
    Europa, another of Jupiter's Galilean moons, is believed to harbor a saltwater ocean beneath a thick icy crust. Scientists believe this ocean could potentially contain twice as much water as all of Earth's oceans combined. Europa is a prime target in the search for extraterrestrial life because of this potential liquid water environment.



## LiteLLM 

LiteLLM is a Python library and SDK designed to simplify the integration and usage of various large language model 
(LLM) APIs. It provides a unified interface to access over 100 LLM services from providers such as OpenAI, Azure, 
HuggingFace, Cohere, and more.  OpenAI compatible  


Below is same version of "tell me about jupiter.  



from litellm import completion

## COMPLETION CALL
response = completion(
  model="vertex_ai/gemini-2.0-flash-001",
  messages=[{
             "content": "Tell me three facts about Jupiter",
             "role": "user"
             }],
)

print(response.to_json(indent=2))
print("---")
Markdown(response.choices[0].message.content)

{
  "id": "chatcmpl-952d3d5c-1e77-49b6-939d-f496183c7764",
  "created": 1740669917,
  "model": "gemini-2.0-flash-001",
  "object": "chat.completion",
  "system_fingerprint": null,
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "Okay, here are three facts about Jupiter:\n\n1.  **Jupiter is the largest planet in our solar system:** It's so big that you could fit all the other planets inside it. Its diameter is about 11 times that of Earth.\n\n2.  **Jupiter has a Great Red Spot:** This is a giant storm that's been raging for at least 350 years. It's larger than the Earth!\n\n3.  **Jupiter has a strong magnetic field:** This magnetic field is about 20,000 times stronger than Earth's, and it creates intense radiation belts around the planet.\n",
        "role": "assistant",
        "tool_calls": null,
        "function_call": null
      }
    }
  ],
  "usage": {
    "completion_tokens": 133,
    "prompt_tokens": 6,
    "total_tokens": 139,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  },
  "vertex_ai_grounding_metadata": [],
  "vertex_ai_safety_results": [],
  "vertex_ai_citation_metadata": []
}
---

Okay, here are three facts about Jupiter:

    Jupiter is the largest planet in our solar system: It's so big that you could fit all the other planets inside it. Its diameter is about 11 times that of Earth.

    Jupiter has a Great Red Spot: This is a giant storm that's been raging for at least 350 years. It's larger than the Earth!

    Jupiter has a strong magnetic field: This magnetic field is about 20,000 times stronger than Earth's, and it creates intense radiation belts around the planet.




## Can set up a list of models to route from as well as using a fallback. 
## Below, vall back is configured to use llama 3.1 if gemini unavailable


from litellm import Router
import json

model_list = [{ # list of model deployments
    "model_name": "gemini_2.0", # model alias -> loadbalance between models with same `model_name`
    "litellm_params": { # params for litellm completion/embedding call
        "model": "vertex_ai/gemini-2.0-flash-001", # actual model name
        "vertex_project": PROJECT_ID,
        "vertex_location": "us-central1"
    }
 },
 {
    "model_name": "gemini_2.0", # model alias -> loadbalance between models with same `model_name`
    "litellm_params": { # params for litellm completion/embedding call
        "model": "vertex_ai/gemini-2.0-flash-001", # actual model name
        "vertex_project": PROJECT_ID,
        "vertex_location": "eu-west4"
    }
 },
 {
    "model_name": "llama_3.1",
    "litellm_params": { # params for litellm completion/embedding call
        "model": "vertex_ai/meta/llama-3.1-8b-instruct-maas",
        "vertex_project": PROJECT_ID,
        "vertex_location": "us-central1"
    }
 }]


# Configure fallback
fallbacks=[{"gemini_2.0": ["llama_3.1"]}] # ðŸ‘ˆ KEY CHANGE

# Initialize our router
router = Router(model_list=model_list, fallbacks=fallbacks)

# This is analogous to openai.ChatCompletion.create
# Requests with model="gemini_2.0" will pick a deployment where model_name="vertex_ai/gemini-2.0-flash-001"
response = router.completion(
    model="gemini_2.0",
    messages=[{"role": "user", "content": "Tell me 3 facts about Jupiter?"}],
    mock_testing_fallbacks=False
    )

print(response.to_json(indent=2))
print("---")
Markdown(response.choices[0].message.content)


## Gen AI SDK supports Pydantic models for structured output
## 

from typing import List
from pydantic import BaseModel, Field

class Planet(BaseModel):
  name: str = Field(description="The name of the planet")
  moons: int = Field(description="The number of moons this planet contains or '0' if none.", default=0)

class SolarSystem(BaseModel):
    planets: List[Planet] = Field(..., description="A python list of all planets in the solar system")

# Display the model's JSON schema
SolarSystem.model_json_schema()


## NOTICE:  Here setting the generated content config to be SolarSystem which is a pydantic class (schema).  
##          The attributes of the pydantic model become part of the prompt
##          The response from the model returns as the Pytdantic shema model
##          
response = client.models.generate_content(
    model="gemini-2.0-flash-001",
    contents="List all the planets in the solar system and the number of moons.",
    config=types.GenerateContentConfig(
        response_mime_type='application/json',
        response_schema=SolarSystem,
    ),
)


# The prompt is effectively:
# List all planets and moons and cast each into an instance of Planet where
# name="planet name" and moons="number of moons" and then add those to a SolarSystem
# which is a list of Planets

# TODO: Output complete prompt

# Notice that response.parsed has already been cast as an instance of "SolarSystem"
solar_system = response.parsed
print(f"solar_system is an instance of: {solar_system.__class__.__name__}")
print("---")

# We can now iterate through all the planets
print("Planet List:")
[print(f"{p.name}, {p.moons} moons") for p in solar_system.planets]

# Optionally print out the de-serialized response for education purposes
print("---")
# print(response.model_dump_json(indent=2))




solar_system is an instance of: SolarSystem
---
Planet List:
Mercury, 0 moons
Venus, 0 moons
Earth, 1 moons
Mars, 2 moons
Jupiter, 95 moons
Saturn, 146 moons
Uranus, 27 moons
Neptune, 16 moons
---


## Function Calling
## Gen AI SDK can call a function on the client
## here the function to call is get_current_weather
## notice in the tools= config section of the GenerateContentConfig is set to get_current_weather
## NOTE:  The docstring of the function gets sent with the prompt 
## NOTE2: this behavior is agentic-like


from google.genai import types
import random

def get_current_weather(location: str) -> str:
    """Returns the current weather.

    Args:
      location: The city and state, e.g. San Francisco, CA
    """

    weather_conditions = [
        "sunny",
        "cloudy",
        "rainy",
        "windy",
        "snowy",
        "foggy",
        "stormy",
        "partly cloudy",
        "drizzling",
        "hailing",
        "thunderstorm",
        "overcast",
        "clear sky",
        "light rain",
        "heavy rain",
        "light snow",
        "heavy snow",
        "blizzard",
        "hurricane",
        "tornado",
    ]

    conditions = random.choice(weather_conditions)

    print(f"Function called: The weather for {location} is {conditions}")
    return conditions


response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents='What is the weather like in Boston?',
    config=types.GenerateContentConfig(
        system_instruction="Speak like an excitable weather announcer.",
        tools=[get_current_weather],
    ),
)

print(response.text)


## Here System Prompt is saying to use the function took to look up the weather for multiple cities
## The Prompt then gives the cities to lookup.

system_instruction = """
Use the configured function tools to lookup the weather.
Speak like an excitable weather announcer.
Return your final response in Markdown using bullets if multiple cities are requested.
"""

prompt="""
What is the weather like in Boston, Philadelphia, Nashville, Atlanta,
Houston, Chicago, Salt Lake City, Seattle, Las Vegas, and San Francisco?
"""

response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents=prompt,
    config=types.GenerateContentConfig(
        system_instruction=system_instruction,
        tools=[get_current_weather],
    ),
)
print("---")
Markdown(response.text)


## Grouding with Google Search
## i.e. utilizes google search as a RAG to return info
## NOTE:  tools in config utilizing GoogleSearch 
## The model is a snapshot of information, Grounding (i.e. GoogleSearch) allows for RAG

from google.genai.types import Tool, GoogleSearch

response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents='What are the 3-star Michelin-rated restaurants in Barcelona? Include 1 fact about each.',
    config=types.GenerateContentConfig(
        tools=[Tool(google_search=GoogleSearch())], # <== Note the use of the GoogleSearch tool
    ),
)

Markdown(response.text)


As of February 27, 2025, Barcelona boasts four restaurants with the prestigious 3-star Michelin rating:

    ABaC: Chef Jordi Cruz is at the helm, known as the youngest Spanish chef to ever receive a Michelin star.
    Cocina Hermanos Torres: Run by the Torres brothers.
    Disfrutar: This restaurant is run by three former chefs from elBulli - Oriol Castro, Eduard Xatruch, and Mateu CasaÃ±as and was named the second-best restaurant in the world in 2023 by "The World's 50 Best Restaurants".
    Lasarte: The first restaurant in Barcelona to be awarded three Michelin stars in 2017. It is guided by the culinary vision of MartÃ­n Berasategui and run by chef Paolo Casagrande. The restaurant is located within the Monument Hotel 5* GL, the only hotel in Spain to boast four Michelin stars: 3 Michelin Stars at Lasarte and 1 Michelin Star at Oria.




## Multimodal Prompting
## utilizes imagen model (model='imagen-3.0-generate-002') 
ex. 
https://googleapis.github.io/python-genai/index.html#imagen




# Generate Image
img_response = client.models.generate_images(
    model='imagen-3.0-generate-002',
    prompt='An umbrella in the foreground, and a rainy night sky in the background',
    config=types.GenerateImagesConfig(
        negative_prompt='human',
        number_of_images=1,
        include_rai_reason=True,
        output_mime_type='image/jpeg',
    ),
)
img_response.generated_images[0].image.show()




## Here image prompt is enhanced via contents prompt
## First is uses gemini 2.0 flash to enhance the original prompt and 
## then sends the enhanced prompt to imagen
## 

from google import genai
from google.genai import types
from google.genai.types import ToolConfig, FunctionCallingConfig, GenerateContentConfig, FunctionCallingConfigMode
from IPython.display import Markdown

# Ask Gemini to enhance a prompt
image_prompt = "An umbrella in the foreground, and a rainy night sky in the background"

response = client.models.generate_content(
    model="gemini-2.0-flash-001",
    contents="Enhance this image prompt, be very creative, provide only a single enhanced prompt: " + image_prompt,
)

enhanced_prompt = response.text

# Output the original and enhanced prompts
print(f"Original Prompt: {image_prompt}")
print(f"Enhanced Prompt: {enhanced_prompt}")


# Use the enhanced prompt to generate the image
img_response = client.models.generate_images(
    model='imagen-3.0-generate-002',
    prompt=enhanced_prompt,
    config=types.GenerateImagesConfig(
        negative_prompt='human',
        number_of_images=1,
        include_rai_reason=True,
        output_mime_type='image/jpeg',
    ),
)
img_response.generated_images[0].image.show()


Original Prompt: An umbrella in the foreground, and a rainy night sky in the background
Enhanced Prompt: **Ethereal Umbrella Canopy: A bioluminescent jellyfish blooms beneath a 
       torrential downpour, its translucent bell a shimmering, inverted umbrella sheltering a 
       miniature cyberpunk cityscape nestled on its surface. Above, the rainy night sky crackles 
       with neon-streaked lightning illuminating monolithic skyscrapers carved from obsidian, 
       their surfaces reflecting the pulsating glow of the jellyfish below. Raindrops, 
       each a tiny prism of refracted light, cascade down, blurring the lines between reality and dream. 
       A single, silhouetted figure stands atop the jellyfish, gazing up at the chaotic beauty, 
       their form barely discernible against the vibrant canvas of storm and shimmering bioluminescence.**


## Multimodal structured prompt.  using the bytes of an image followed by a text prompt to describe the image
## 

# Here, we use Part.from_bytes() to send the image that was generated in the previous cell.
# Note that we need to provide both the image bytes and the mime_type.
prompt = [
        types.Part.from_bytes(
            data=img_response.generated_images[0].image.image_bytes,
            mime_type=img_response.generated_images[0].image.mime_type
            ),
        "Describe the above image"
        ]

# Send the request
response = client.models.generate_content(
    model="gemini-2.0-flash-001",
    contents=prompt
)

Markdown(response.text)


## Multimodal structured prompt.  
##  Here a HTML URL is retrieved and the contents are then summarized in the prompt

import requests

html_url="https://arxiv.org/html/1706.03762v7"

# Get the contents of the HTML URL using the Python Requests library
content = requests.get(html_url).content

# Create a multi-part prompt
prompt = [
    content,
    "Summarize the above content"
]

# Generate our response
response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt
)

Markdown(response.text)

## Multimodal structured prompt part 2
## Here a URL is retrieved which is a PDF and the contents are then summarized in the prompt
## NOTICE the mime_type in the pdf_part


pdf_url = "https://arxiv.org/pdf/1706.03762"

# Get the contents of the PDF URL using the Python Requests library
content = requests.get(pdf_url).content

pdf_part = types.Part.from_bytes(
      data=content,
      mime_type="application/pdf" # <=== Set the MIME TYPE to 'application/pdf'
    )
# Create a multi-part prompt
prompt = [
    pdf_part,
    "Summarize the above content"
]

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt
)

Markdown(response.text)


## Here playwrite is used to visit the URL and saved to a file

# Use playwright to visit a URL and take a screenshot of the entire page which is saved to a local file.

from playwright.async_api import async_playwright
from playwright.sync_api import sync_playwright
import playwright
import asyncio
from IPython.display import Image

# Colab runs in an async runloop, so we'll write this as an async function
async def fetch_page_image(url: str = ""):
    async with async_playwright() as p:
        # Generate a filename using the url, but use only the domain and path, delimited by an underscore (_)
        filename = url.replace("https://", "").replace("http://", "").replace("/", "_") + ".png"
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto(url)
        await page.screenshot(path=f"{filename}", full_page=True)
        # Get the HTML content
        html_content = await page.content()
        await browser.close()
        # Return the image bytes and HTML content
        return filename, open(f"{filename}", "rb").read(), html_content


image_filename, image_bytes, html_content = await fetch_page_image("https://arxiv.org/html/1706.03762v7")

# Display the image in path "example.png" but scaled to 100 pixels wide
Image(filename=image_filename, width=800)


## get HTML and image and send via pydantic model (ArxivExtractionModel)
##  page_sumary:
##  authors:
##  references:
## 
##  here the prompt is image and html content
##  the config sets the response_schema as Pytdantic Model ArxivExtractionModel
## 

# Upload the file using the API

from pydantic import BaseModel, Field
from google.cloud import storage
from google import genai
from google.genai import types
import pprint

class ArxivExtractionModel(BaseModel):
  page_summary: str = Field(description="A summary of the provided webpage")
  authors: list[str] = Field(description="The name(s) of the authors.")
  references: list[str] = Field(description="Every reference cited in the paper")

# If using GCP (PROJECT_ID is defined), create a "Part" from the image bytes we already have in memory,
# otherwise (PROJECT_ID is empty), we're using AI Studio, so upload the file using the
if PROJECT_ID != "":

  if BUCKET == "":
    # Create image part from existing bytes stored in memory
    image_part = types.Part.from_bytes(data=image_bytes, mime_type="image/png")
  else:
    # Alternative: Upload the file to GCS and use a Part.from_uri
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(BUCKET)
    blob = bucket.blob(image_filename)
    blob.upload_from_filename(image_filename)

    gcs_uri = f'gs://{bucket.name}/{blob.name}'

    image_part = types.Part.from_uri(
      file_uri=gcs_uri,
      mime_type="image/png"
    )

else:
  file_upload = client.files.upload(path=image_filename)
  image_part=types.Part.from_uri(
                    file_uri=file_upload.uri,
                    mime_type=file_upload.mime_type)

# Use a multi-part structured prompt
prompt = ["# Web Page Image",
          image_part,
          "# Web Page HTML Content",
          html_content,
          "Summarize the web page screenshot and HTML content. Then condense the information contained within. format everyting in Markdown."
          ]

config = types.GenerateContentConfig(
    response_schema=ArxivExtractionModel,
    response_mime_type="application/json",
    system_instruction=None
)

response = client.models.generate_content(
    model=MODEL_ID,
    contents=prompt,
    config=config,
)

# Parse the structured response. This can be done using `response.parsed` (Method 1)
# or through explicit casting/model validation (Method 2)

# Method 1: Use "response.parsed" to get the response cast into ArxivExtractionModel
parsed_response = response.parsed

# Method 2: Use explicit casting (not used, but equivalent to Method 1)
cast_response = ArxivExtractionModel.model_validate_json(response.text)

# Print the results
print(parsed_response.model_dump_json(indent=2))
print("---")
Markdown(parsed_response.page_summary)



## CrewAI Agent
## This sample CrewAI agent first generates a list of planets, then randomly selects one 
## before sharing 3 facts about the selected planet.
## 

from typing import Type, List, Optional
from pydantic import BaseModel, Field
from crewai import LLM, Task, Agent, Crew
from crewai.tools import BaseTool
import random
from IPython.display import display, Markdown


class SolarSystem(BaseModel):
    planets: List[str] = Field(..., description="A python list containing planet names")

class PlanetSelectorTool(BaseTool):
  name: str = "random_planet_selector"
  description: str = "Select a random planet from an SolarSystem instance"
  args_schema: Type[BaseModel] = SolarSystem

  def _run(self, planets: List[str] ) -> str:
    planet = random.choice(planets)
    display(Markdown(f"Selected planet: {planet}"))
    return planet

tool_planet_selector = PlanetSelectorTool()

llm = LLM(
    model="vertex_ai/gemini-2.0-flash-001",
)

# Create an agent using the custom LLM
agent = Agent(
    role="AI Assistant",
    goal="Provide helpful responses",
    backstory="I am an AI assistant created to help users.",
    llm=llm,
    verbose=True
)

task_planet_list=Task(
    description="List all planets in the solar system",
    expected_output="A list of planets",
    output_pydantic=SolarSystem,
    agent=agent
)

task_planet_selector=Task(
    description="Select a random planet from this list:",
    expected_output="The name of the selected planet",
    agent=agent,
    tools=[tool_planet_selector],
    context=[task_planet_list]
)

# Create tasks for the agent
task_facts = Task(
    description="Provide 3 facts about the following planet",
    expected_output="Respond to user queries",
    agent=agent,
    context=[task_planet_selector]
)

# Create the crew with the agent and task
crew = Crew(
    agents=[agent],
    tasks=[task_planet_list, task_planet_selector, task_facts],
)

response = crew.kickoff()

Markdown(response.raw)












.